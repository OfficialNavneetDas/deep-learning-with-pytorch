{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afa3dd3-f6f7-4d08-a2c0-1d9685c75459",
   "metadata": {},
   "source": [
    "# Import Sequential dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e1410e-fd6b-4500-834e-c347c6fda5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question      answer\n",
       "0                   What is the capital of France?       Paris\n",
       "1                  What is the capital of Germany?      Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
       "3  What is the largest planet in our solar system?     Jupiter\n",
       "4   What is the boiling point of water in Celsius?         100"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"100_Unique_QA_Dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b6cd5-f528-4581-8551-d19a5506a7a2",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ece570-9a74-4614-8f7e-be7f0c7dc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"?\",\"\")\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3128e53b-7557-4edf-91c8-07c8dec3ce60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'capital', 'of', 'france']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3cd94b-edca-4a4a-bd56-dd4c9e02c498",
   "metadata": {},
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a43efb-8610-46b5-a947-2beacc7c12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"<UKN>\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa897ec1-8ff1-4bb4-a1e4-d7001e938152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row):\n",
    "    tokenized_question = tokenize(row[\"question\"])\n",
    "    tokenized_answer = tokenize(row[\"answer\"])\n",
    "\n",
    "    merged_tokens = tokenized_question + tokenized_answer\n",
    "\n",
    "    for token in merged_tokens:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b884c1-6727-4de6-a9ba-cf8c6d915693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.apply(build_vocab, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "742b6842-5c32-4c0b-bebc-626724bec32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UKN>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " 'to': 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " 'mockingbird': 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " '1984': 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " 'starry': 130,\n",
       " 'night': 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'step': 143,\n",
       " 'moon': 144,\n",
       " 'armstrong': 145,\n",
       " 'main': 146,\n",
       " 'ingredient': 147,\n",
       " 'guacamole': 148,\n",
       " 'avocado': 149,\n",
       " 'sides': 150,\n",
       " 'does': 151,\n",
       " 'hexagon': 152,\n",
       " 'have': 153,\n",
       " '6': 154,\n",
       " 'china': 155,\n",
       " 'yuan': 156,\n",
       " 'pride': 157,\n",
       " 'and': 158,\n",
       " 'prejudice': 159,\n",
       " 'jane-austen': 160,\n",
       " 'iron': 161,\n",
       " 'fe': 162,\n",
       " 'hardest': 163,\n",
       " 'natural': 164,\n",
       " 'substance': 165,\n",
       " 'diamond': 166,\n",
       " 'continent': 167,\n",
       " 'by': 168,\n",
       " 'area': 169,\n",
       " 'asia': 170,\n",
       " 'president': 171,\n",
       " 'states': 172,\n",
       " 'george-washington': 173,\n",
       " 'bird': 174,\n",
       " 'its': 175,\n",
       " 'ability': 176,\n",
       " 'mimic': 177,\n",
       " 'sounds': 178,\n",
       " 'parrot': 179,\n",
       " 'longest-running': 180,\n",
       " 'animated': 181,\n",
       " 'tv': 182,\n",
       " 'show': 183,\n",
       " 'simpsons': 184,\n",
       " 'vaticancity': 185,\n",
       " 'most': 186,\n",
       " 'moons': 187,\n",
       " 'saturn': 188,\n",
       " 'romeo': 189,\n",
       " 'juliet': 190,\n",
       " 'shakespeare': 191,\n",
       " 'earths': 192,\n",
       " 'atmosphere': 193,\n",
       " 'nitrogen': 194,\n",
       " 'bones': 195,\n",
       " 'adult': 196,\n",
       " 'human': 197,\n",
       " 'body': 198,\n",
       " '206': 199,\n",
       " 'metal': 200,\n",
       " 'liquid': 201,\n",
       " 'at': 202,\n",
       " 'room': 203,\n",
       " 'temperature': 204,\n",
       " 'mercury': 205,\n",
       " 'russia': 206,\n",
       " 'moscow': 207,\n",
       " 'electricity': 208,\n",
       " 'benjamin-franklin': 209,\n",
       " 'second-largest': 210,\n",
       " 'land': 211,\n",
       " 'color': 212,\n",
       " 'ripe': 213,\n",
       " 'banana': 214,\n",
       " 'yellow': 215,\n",
       " 'month': 216,\n",
       " '28': 217,\n",
       " 'days': 218,\n",
       " 'common': 219,\n",
       " 'february': 220,\n",
       " 'study': 221,\n",
       " 'living': 222,\n",
       " 'organisms': 223,\n",
       " 'called': 224,\n",
       " 'biology': 225,\n",
       " 'home': 226,\n",
       " 'great': 227,\n",
       " 'wall': 228,\n",
       " 'bees': 229,\n",
       " 'collect': 230,\n",
       " 'from': 231,\n",
       " 'flowers': 232,\n",
       " 'nectar': 233,\n",
       " 'opposite': 234,\n",
       " 'day': 235,\n",
       " 'south': 236,\n",
       " 'korea': 237,\n",
       " 'seoul': 238,\n",
       " 'bulb': 239,\n",
       " 'edison': 240,\n",
       " 'humans': 241,\n",
       " 'breathe': 242,\n",
       " 'survival': 243,\n",
       " 'oxygen': 244,\n",
       " '144': 245,\n",
       " '12': 246,\n",
       " 'pyramids': 247,\n",
       " 'giza': 248,\n",
       " 'egypt': 249,\n",
       " 'sea': 250,\n",
       " 'creature': 251,\n",
       " 'eight': 252,\n",
       " 'arms': 253,\n",
       " 'octopus': 254,\n",
       " 'holiday': 255,\n",
       " 'celebrated': 256,\n",
       " 'december': 257,\n",
       " '25': 258,\n",
       " 'christmas': 259,\n",
       " 'yen': 260,\n",
       " 'legs': 261,\n",
       " 'spider': 262,\n",
       " 'sport': 263,\n",
       " 'uses': 264,\n",
       " 'net,': 265,\n",
       " 'ball,': 266,\n",
       " 'hoop': 267,\n",
       " 'basketball': 268,\n",
       " 'kangaroos': 269,\n",
       " 'female': 270,\n",
       " 'minister': 271,\n",
       " 'uk': 272,\n",
       " 'margaretthatcher': 273,\n",
       " 'fastest': 274,\n",
       " 'animal': 275,\n",
       " 'cheetah': 276,\n",
       " 'periodic': 277,\n",
       " 'table': 278,\n",
       " 'spain': 279,\n",
       " 'madrid': 280,\n",
       " 'closest': 281,\n",
       " 'sun': 282,\n",
       " 'father': 283,\n",
       " 'computers': 284,\n",
       " 'charlesbabbage': 285,\n",
       " 'mexico': 286,\n",
       " 'mexicocity': 287,\n",
       " 'colors': 288,\n",
       " 'rainbow': 289,\n",
       " 'musical': 290,\n",
       " 'instrument': 291,\n",
       " 'black': 292,\n",
       " 'white': 293,\n",
       " 'keys': 294,\n",
       " 'piano': 295,\n",
       " 'americas': 296,\n",
       " '1492': 297,\n",
       " 'christophercolumbus': 298,\n",
       " 'disney': 299,\n",
       " 'character': 300,\n",
       " 'long': 301,\n",
       " 'nose': 302,\n",
       " 'grows': 303,\n",
       " 'it': 304,\n",
       " 'when': 305,\n",
       " 'lying': 306,\n",
       " 'pinocchio': 307,\n",
       " 'directed': 308,\n",
       " 'movie': 309,\n",
       " 'titanic': 310,\n",
       " 'jamescameron': 311,\n",
       " 'superhero': 312,\n",
       " 'also': 313,\n",
       " 'dark': 314,\n",
       " 'knight': 315,\n",
       " 'batman': 316,\n",
       " 'brasilia': 317,\n",
       " 'fruit': 318,\n",
       " 'king': 319,\n",
       " 'fruits': 320,\n",
       " 'mango': 321,\n",
       " 'eiffel': 322,\n",
       " 'tower': 323}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd3e6c6d-92a7-4b83-80ee-e897c1cca377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16244505-7f1e-45e4-9ea0-61010644a581",
   "metadata": {},
   "source": [
    "# Convert Word To Numerical Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6c1537c-9d07-44af-afb5-3156bf83b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indeces(text,vocab):\n",
    "    indexed_text=[]\n",
    "    for token in tokenize(text):\n",
    "        if token in vocab:\n",
    "            indexed_text.append(vocab[token])\n",
    "        else:\n",
    "            indexed_text.append(vocab[\"<UKN>\"])\n",
    "    return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95d0bc1a-0d05-4acb-a050-e6d4ba69b914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 24, 25, 5, 26, 19, 27]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indeces(\"What is the boiling point of water in Celsius?\",vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca5aef8d-88c6-4a99-a285-be2e06fb84fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "959d9a0d-8c9a-4d11-bef1-fdbdf291e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAdataset(Dataset):\n",
    "    def __init__(self,df,vocab):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        numerical_question = text_to_indeces(self.df.iloc[index][\"question\"],self.vocab)\n",
    "        numerical_answer = text_to_indeces(self.df.iloc[index][\"answer\"],self.vocab)\n",
    "\n",
    "        return torch.tensor(numerical_question), torch.tensor(numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "211357f5-1c6c-4b87-a16c-95f93d371d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QAdataset(data,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b53cfe57-bdae-4cbc-b62d-822a3c41833e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3, 4, 5, 8]), tensor([9]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce6fb4f0-b205-47ef-9aa8-ddeff2b3c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloasder = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18a376-4f8a-4dab-83fe-10920994a4ed",
   "metadata": {},
   "source": [
    "# making RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83edd489-97e2-4cc9-9cc9-010d25610074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "625f2e94-8ae7-48af-ba86-8d319f2ed847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim=50)\n",
    "        self.RNN = nn.RNN(50,64,batch_first=True)\n",
    "        self.fc = nn.Linear(64,vocab_size)\n",
    "        \n",
    "    def forward(self,question):\n",
    "        embedding_question = self.embedding(question)\n",
    "        hidden, final = self.RNN(embedding_question)\n",
    "        output = self.fc(final.squeeze(0))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c968e2f3-fe4a-44b7-bea6-e6f7f4a41e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2377e831-aa85-4358-ab17-763d416c3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "892c241e-5c0c-4eff-b66f-d322d7c63512",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lossfunction = nn.CrossEntropyLoss()\n",
    "optimzer = torch.optim.Adam(model.parameters(), lr = Learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd400f83-dc1d-4380-aaa2-cab011ab7966",
   "metadata": {},
   "source": [
    "# traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da8d6584-0539-448e-9737-6ac4c056fab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Loss: 5.86321685579088 , total_epoch_loss:527.6895170211792\n",
      "Epoch: 2 , Loss: 5.145487308502197 , total_epoch_loss:463.09385776519775\n",
      "Epoch: 3 , Loss: 4.31343506442176 , total_epoch_loss:388.2091557979584\n",
      "Epoch: 4 , Loss: 3.6112950457466972 , total_epoch_loss:325.01655411720276\n",
      "Epoch: 5 , Loss: 3.0176745229297213 , total_epoch_loss:271.5907070636749\n",
      "Epoch: 6 , Loss: 2.473293720351325 , total_epoch_loss:222.59643483161926\n",
      "Epoch: 7 , Loss: 1.9613465944925943 , total_epoch_loss:176.5211935043335\n",
      "Epoch: 8 , Loss: 1.5216200596756406 , total_epoch_loss:136.94580537080765\n",
      "Epoch: 9 , Loss: 1.1605187389585707 , total_epoch_loss:104.44668650627136\n",
      "Epoch: 10 , Loss: 0.8804084440072377 , total_epoch_loss:79.2367599606514\n",
      "Epoch: 11 , Loss: 0.671313097079595 , total_epoch_loss:60.418178737163544\n",
      "Epoch: 12 , Loss: 0.5215747036867672 , total_epoch_loss:46.941723331809044\n",
      "Epoch: 13 , Loss: 0.40984146330091686 , total_epoch_loss:36.88573169708252\n",
      "Epoch: 14 , Loss: 0.3300449719031652 , total_epoch_loss:29.704047471284866\n",
      "Epoch: 15 , Loss: 0.2700588128632969 , total_epoch_loss:24.305293157696724\n",
      "Epoch: 16 , Loss: 0.2227240832315551 , total_epoch_loss:20.045167490839958\n",
      "Epoch: 17 , Loss: 0.1871755694349607 , total_epoch_loss:16.84580124914646\n",
      "Epoch: 18 , Loss: 0.1593191958963871 , total_epoch_loss:14.338727630674839\n",
      "Epoch: 19 , Loss: 0.13570830474297205 , total_epoch_loss:12.213747426867485\n",
      "Epoch: 20 , Loss: 0.1180724216832055 , total_epoch_loss:10.626517951488495\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_epoch_loss = 0\n",
    "    for question,answer in dataloader:\n",
    "        \n",
    "        # forword pass\n",
    "        outputs = model(question)\n",
    "        \n",
    "        # calcluate loss\n",
    "        loss = Lossfunction(outputs, answer[0])\n",
    "        \n",
    "        # back prop\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # upgrade grads\n",
    "        optimzer.step()\n",
    "\n",
    "        total_epoch_loss = total_epoch_loss + loss.item()\n",
    "    # priniting the avrage loss per epoch\n",
    "    avg_loss = total_epoch_loss/len(dataloader)\n",
    "    print(f\"Epoch: {epoch+1} , Loss: {avg_loss} , total_epoch_loss:{total_epoch_loss}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d2eef80-f1e2-4a58-a4be-352e9ab2f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,question,threshold = 0.5):\n",
    "    num_question = text_to_indeces(question,vocab)\n",
    "\n",
    "    question_tensor = torch.tensor(num_question).unsqueeze(0)\n",
    "\n",
    "    output = model(question_tensor)\n",
    "\n",
    "    probs =torch.nn.functional.softmax(output)\n",
    "\n",
    "    value, index = torch.max(probs,dim=1)\n",
    "\n",
    "    if value<threshold:\n",
    "        print(\"I Don't Know\")\n",
    "    else:\n",
    "        print(list(vocab.keys())[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afe76a7d-2f93-4f5c-919b-36864b132632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\updes\\AppData\\Local\\Temp\\ipykernel_4164\\625736601.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs =torch.nn.functional.softmax(output)\n"
     ]
    }
   ],
   "source": [
    "predict(model,\"What is the boiling point of water in Celsius\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
